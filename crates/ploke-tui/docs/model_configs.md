# Model Configs

NOTE: Most of this is AI-generated by KimiK2, DeepSeekv3-0325, and DeepSeek R1T2 Chimera
- Joseph LeBlanc (Jul 17 2025)

## Steps forward

### Review of Current Implementation

**What's already built:**
- ✅ Single-provider configuration (`ProviderConfig`) with OpenRouter support
- ✅ Basic request formatting for OpenRouter API
- ✅ Environment-based API key loading
- ✅ Model specification via config

**What needs modification:**

1. **Configuration Schema**: Current `ProviderConfig` is single-provider. Need to evolve to:
   ```rust
   pub struct Config {
       pub providers: Vec<ProviderConfig>,
       pub default_provider: String,
       pub aliases: HashMap<String, String>, // For shortcuts like "!gpt"
   }
   ```

2. **Registry System**: Need a `ProviderRegistry` that can:
   - Store multiple provider configs
   - Handle provider-specific request/response formats
   - Manage rate limiting per provider
   - Track usage/costs

3. **UI Commands**: Add chat commands like:
   - `/model list` - show available models
   - `/model use <model>` - switch active model
   - `/model alias <shortcut> <full-model-name>` - create aliases

### Implementation Plan Assessment

**Good starting points:**
- OpenRouter-first approach is smart - gives broad model access through single API
- Single config.toml is user-friendly
- Per-message switching with aliases is powerful UX

**Recommended approach:**
1. **Phase 1**: Extend `ProviderConfig` to support multiple providers in array
2. **Phase 2**: Build lightweight registry with provider-specific formatters
3. **Phase 3**: Add chat commands for switching models
4. **Phase 4**: Add cost tracking and fallback mechanisms

**Minimal changes needed:**
- `ProviderConfig` → `Vec<ProviderConfig>` 
- Add model switching commands to command parser
- Create provider registry in `llm` module
- Update request formatting to be provider-aware

The current architecture is actually well-positioned for this - the separation between `ProviderConfig` and the LLM manager means we can add multi-provider support without major refactoring.

## Initial Plan

                       Additional steps needed for multi-model support:

 1 - [ ] Model Configuration Schema - Add a way to define multiple model endpoints in config
 2 - [ ] Provider Registry - Create a registry system to handle different API providers (OpenAI,
     Anthropic, OpenRouter, etc.)
 3 - [ ] Model-Specific Request Formatters - Each provider has different request/response formats
 4 - [ ] Provider Selection UI - Add commands to switch between configured models
 5 - [ ] Model Capability Detection - Handle different max tokens, context windows, etc.
 6 - [ ] Rate Limiting per Provider - Different providers have different rate limits
 7 - [ ] Error Handling per Provider - Different error codes and retry strategies
 8 - [ ] Streaming Support Variations - Not all providers support streaming the same way
 9 - [ ] Cost Tracking - Track usage/cost per provider
10 - [ ] Fallback Mechanism - Switch to backup providers on failure


                                 Questions for clarification:

1 Priority: Should we focus on OpenRouter (which already supports many models via one API)
  first, or build direct support for specific providers?
  - Answer: Yes, we should focus on OpenRouter first.
2 Configuration: Do you want model configs in the main config.toml, or separate model-specific
  config files?
  - Answer: We want a single config.toml that has the specs for all the models.
3 Switching: Should users be able to switch models per-conversation, or only at startup?
  - Answer: They should be able to switch per-message using a chat command.
  - 3.1 (Switching) For per-message switching: Should we support model "aliases" (e.g., `!gpt` = `openrouter/gpt-4-turbo`)?  
     - Answer Yes, with user-definable aliases in config  
4 Defaults: Should we maintain a curated list of "recommended" models with sensible defaults?
  - Answer: Yes, this is important.
5 Environment: Should API keys be per-model or shared across providers?
  - Answer: Shared across providers.

The current `ProviderConfig` in user_config.rs seems designed for a single provider. Should we
evolve this into a `Vec<ProviderConfig>` with a way to select the active one?

More information on future development is in `ploke-tui/docs/model_configs.md`

## Tracking changes

### Commit 05ca190 
aider: feat: add ProviderRegistry for multi-model API support

These changes introduce:

 1 ProviderRegistry to manage multiple providers
 2 Each provider has a unique ID and optional display name
 3 Support for provider-specific request formatting via ProviderType
 4 Aliases for quick model switching
 5 Backward compatibility with the default provider setup

---

## Additional comments from Deepseek R1T2 Chimera

### **Additional Checklist Items**

11. **Authentication Variance Handling**  
    - [ ] Different providers use different auth schemes (API keys vs OAuth vs tokens)
    - [ ] Secure credential storage/retrieval mechanisms per provider

12. **Tokenization Alignment**  
    - [ ] Model-specific tokenizers to calculate precise context window usage  
    - [ ] Automatic prompt truncation when exceeding model limits

13. **Context Window Management**  
    - [ ] Conversation history summarization/trimming strategies for models with smaller context windows  
    - [ ] Explicit context size warnings when switching between models mid-conversation

14. **Input Validation Layers**  
    - [ ] Provider-specific guardrails (e.g., Claude's XML format preference, LLaMA's strict input policies)

15. **Latency Handling**  
    -[ ] Visual feedback mechanisms for high-latency providers  
    -[ ] Configurable timeouts per provider

16. **Model Metadata Tracking**  
    -[ ] Versioning support (gpt-4-turbo vs gpt-4o, claude-3-opus-2024-05, etc.)  
    -[ ] Special capability flags (tool calling, image input, JSON mode)

17. **Cost Estimation Previews**  
    -[ ] Price calculation display before executing large operations  
    -[ ] Budget cutoff alerts

---

### **Key Technical Recommendations**

**For `user_config.rs` Changes:**
```rust
#[derive(Deserialize, Serialize)]
enum ProviderType {
    OpenRouter,
    OpenAI,
    Anthropic,
    Custom(String) // Future-proofing
}

#[derive(Deserialize, Serialize)]
struct ProviderConfig {
    provider_type: ProviderType,
    endpoint: Url,
    api_key: Option<String>, 
    model_defaults: HashMap<String, Value>, // Vendor-specific params
    priority: u8 // For fallback ordering
}

// In main config:
active_provider: String,
providers: Vec<ProviderConfig>
```

**Critical User Experience Additions:**
1. `/models` command to show available providers + current status
2. Auto-complete for model switching (`/use gpt-` → shows available GPT variants)
3. Session-scoped model indicators in chat header
4. Streaming format adapter interface

---

### **Important Additional Questions**

1. **Conversation Continuity:**  
   *How should chat history be adapted when switching between models with vastly different capabilities (e.g., GPT-4 → Claude Haiku)?*  
   - Partial history rewrites?  
   - Automatic summarization points?  

2. **State Management:**  
   *Should model-specific session states be preserved when switching back to previous providers?*  
   - Maintain separate chat histories per model?  
   - Cross-model context carryover?  

3. **Enterprise Requirements:**  
   *Are there compliance needs for private/on-prem models?*  
   - Data residency guarantees  
   - Disable logging flags  

4. **Extensibility:**  
   *Should we design a plugin system for community-contributed providers?*  
   - WASM-based provider modules?  
   - Configuration hot-reloading?  

5. **Benchmarking:**  
   *Add provider performance metrics to help users choose models?*  
   - Latency/cost/quality tradeoff tracking  

---

### **Security Considerations**
- Audit trail for model access (who used which model when)  
- Sensitive input scanning before sending to third-party providers  
- Per-provider data retention policies  

This architecture would support both the OpenRouter shortcut strategy while enabling future direct integrations. The key is abstracting provider differences while maintaining granular control where needed.

---

## Additional items from DeepSeek V3


### Additional Checklist Items:

**Context Management:**
11 - [ ] Cross-Model Context Retention - Decide how conversation history is shared/switched between models   
12 - [ ] Model-Specific Context Windows - Auto-truncate or summarize when exceeding a model's limits  
13 - [ ] Syntax Highlighting Detection - Some models handle markdown/code blocks differently  

**User Experience:**
14 - [ ] Model Comparison Command - `!compare model1 model2 "question"` to test outputs side-by-side  
15 - [ ] Model Metadata Display - Show token costs, capabilities, and limits when switching  
16 - [ ] Privacy Safeguards - Warn when switching to non-enterprise/non-local models with sensitive code  

**Advanced Features:**
17 - [ ] Model Ensembling - Allow routing queries to multiple models and combining responses  
18 - [ ] Automatic Model Selection - Rules for auto-choosing models based on query type (e.g., Claude for docs, GPT-4 for complex code)  
19 - [ ] Local Model Support - HuggingFace/TGI/Ollama integration alongside cloud providers  

**Configuration:**
20 - [ ] Migratable Sessions - Save/load conversations with model-switching history preserved  

### Refined Questions from Your List:
1. (Priority) For OpenRouter focus: Should we implement provider-specific optimizations later as escape hatches?  
   *Suggested Answer:* Yes, but through a plugin system rather than hardcoded exceptions  

3. (Switching) For per-message switching: Should we support model "aliases" (e.g., `!gpt` = `openrouter/gpt-4-turbo`)?  
   *Suggested Answer:* Yes, with user-definable aliases in config  

5. (Environment) For shared API keys: Should we implement key priority (e.g., try OpenAI key first, fallback to OpenRouter)?  
   *Suggested Answer:* Yes, with clear key precedence rules  

### New Questions:
1. **Context Handling:** When switching models mid-conversation, should we:  
   a) Keep full history (may exceed new model's context)  
   b) Summarize before switching  
   c) Start fresh  
   *Suggested Answer:* Configurable behavior with (b) as default  

2. **Cost Tracking:** Should estimated costs show:  
   a) Per-message  
   b) Running total  
   c) Project-level aggregates  
   *Suggested Answer:* All three, toggleable  

3. **Local vs. Cloud:** Should local models have separate config lanes to avoid rate-limit logic?  
   *Suggested Answer:* Yes, with a `local_models[]` array in config  

4. **Middleware:** Add hooks for:  
   a) Pre-process prompts per-model  
   b) Post-process responses  
   c) Audit logging  
   *Suggested Answer:* All three as optional modules  

### Architectural Suggestion:
Instead of just `Vec<ProviderConfig>`, consider:
```rust
enum ProviderType {
    OpenRouter(OpenRouterConfig),
    Direct(DirectProviderConfig),
    Local(LocalModelConfig) 
}

struct ProviderConfig {
    active: Option<String>, // Currently selected
    available: HashMap<String, ProviderType>,
    fallback_order: Vec<String>
}
```

---


## Model switching:

 1 crates/ploke-tui/src/user_config.rs
   • add a tiny helper so we can mutate the active provider (today it is read-only).
   • expose a canonical list of aliases so the UI can offer tab-completion.
 2 crates/ploke-tui/src/app_state.rs
   • new state-command variants:
   – SwitchModel { alias_or_id: String }
   – ListModels (optional, for help)
   • teach state_manager to update ProviderRegistry::active_provider and broadcast a new
   AppEvent::ModelChanged { alias: String }.
 3 crates/ploke-tui/src/app.rs
   • normal-mode keymap: m (or <space>m) to enter “model picker”.
   • tiny overlay widget that shows the last two segments of the model id, e.g.
   “…gpt-4-turbo” or the alias “gpt4”.
   • flash animation:
   – on ModelChanged event, set a flash_until: Instant in App.
   – in draw() render the widget with bright yellow fg, then fade to dim gray over ~1 s.
   • optional: /model <alias> slash command (reuse the same state-command).
 4 crates/ploke-tui/src/llm/mod.rs
   • make prepare_and_run_llm_call read the current active_provider each time (no caching), so
   the change is honoured immediately.

