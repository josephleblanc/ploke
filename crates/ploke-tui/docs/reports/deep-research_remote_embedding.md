Integrating Hugging Face and OpenAI Embedding APIs
Hugging Face Embeddings API
JSON Request Format
The Hugging Face Inference API for embeddings uses a simple HTTP POST. The URL includes the model ID, for example: https://api-inference.huggingface.co/models/{model_id}. The request body is JSON with an "inputs" field containing the text or list of texts to embed. For example, to embed text using the model "sentence-transformers/all-MiniLM-L6-v2", one can send:
{
  "inputs": ["Text to embed 1", "Text to embed 2"],
  "options": { "wait_for_model": true }
}
along with the header Authorization: Bearer {HF_API_TOKEN}. This will prompt the API to return embeddings for each input string. The wait_for_model option is recommended to handle cold starts – it tells the service to load the model and wait rather than timing out if the model isn’t already loaded in memory.
The Hugging Face API also supports an older format with explicit pipeline in the URL (e.g. .../pipeline/feature-extraction/{model}), but the current recommended usage is the unified /models/{model} endpoint which infers the task from the model’s configuration. No task parameter is needed if the model’s pipeline tag is set (e.g. models intended for embeddings are tagged as "feature-extraction" on the Hub).
Output Format & Embedding Dimensions
The HF API returns embeddings as JSON data, typically as a list of lists of numbers (each sub-list is the vector for one input). The structure can vary slightly by model. For sentence-transformer models, the output is usually a list of n embedding vectors for n inputs. For example, if you send two sentences to "sentence-transformers/all-MiniLM-L6-v2", you’ll receive an array with two inner arrays, each containing 384 floating-point values (since that model maps text to a 384-dimensional dense vector). If a single string is sent, you’ll get a list with one embedding vector.
If the model is not specifically a sentence embedding model, the output may include extra dimensions. For instance, using a sequence-to-sequence model like BART in the feature-extraction pipeline returns a tensor for each input where each token in the text has its own embedding. In JSON this appears as an array of arrays of arrays (basically a 2D matrix for each input). Typically for embeddings use-cases, one would choose models that produce a single vector per input (either because the model itself does pooling, or by configuring the pipeline accordingly).
Vector size: The dimensionality of the embedding vector is determined by the model architecture. It is not something you specify in the API request – the model’s output dimension is fixed. For example, the popular "all-MiniLM-L6-v2" model outputs 384-dimensional vectors, while larger models like "all-mpnet-base-v2" produce 768-dimensional vectors. You can find the embedding size in the model’s documentation or model card on the Hub (it’s often mentioned, as in the MiniLM model card stating 384 dimensions). The API will simply return whatever the model generates. Your application should be prepared to handle different lengths depending on the configured model.
Rate Limits and Usage
Hugging Face’s free Inference API is subject to rate limiting to ensure fair use. As of September 2025, free users are limited to roughly 1,000 API calls per 5-minute window (this equates to about 200 requests per minute, though the enforcement is in a 5-minute bucket). These values can change over time and are sometimes adjusted for platform health. If you authenticate with a free user token, you’ll fall under this quota. Pro and higher-tier accounts have higher limits; for example a Pro user can make about 2,500 requests per 5 minutes, and higher tiers (Team, Enterprise) scale up further.
The free tier also has burst limitations. The documentation indicates Hugging Face favors steady request rates and may throttle bursts even if you stay under the 5-minute quota. In practice, users have observed that the free plan is fairly strict – hitting the limits will result in HTTP 429 "Too Many Requests" errors. Upgrading to a paid plan (Pro or higher) not only grants higher rate limits but also a small compute credit to use for faster inference.
Accelerated Inference vs. Serverless Free API: The default inference API runs on shared, CPU-based workers and loads models on demand. Hugging Face offers an Accelerated Inference option (pay-as-you-go) where you can get priority access to GPU hardware for faster processing. This requires a credit balance (Pro plan includes some free credits, after which usage is billed per second of compute). If you anticipate heavy or real-time usage (e.g., embedding large datasets), using the paid accelerated inference or even a dedicated inference endpoint is recommended to avoid throttling. The good news is that you can batch inputs in a single request: sending an array of texts counts as one request in terms of rate limits, which significantly improves throughput (the example in the forums shows 1,000 comments embedded in ~13 seconds as a single request).
It’s worth noting that Hugging Face does not publish a fixed “requests per second” limit; the system uses the token-based windows described above and may dynamically throttle if needed. Always implement error handling for 429 responses. If you get rate-limited, you might consider spacing out requests or upgrading your plan.
Authentication (Free vs Paid)
API Token: To use the Hugging Face inference API, you need an API token. This token is available from your Hugging Face account settings (a free account can create one). The token must be included as a Bearer token in the Authorization header of your requests. For example: Authorization: Bearer hf_yourTokenHere. Without a token, you will get 401 Unauthorized or be treated as an anonymous user with very restrictive limits.
Token scopes: Hugging Face now supports fine-grained tokens. For inference, a token with read access is sufficient for public models. Ideally, you create a token that has the “Inference API” permission scope (and possibly no broader write permissions if not needed). This ensures security and that the token can only be used to call inference.
Free vs Pro: The same API endpoints are used regardless of free or paid. Your account type is determined by the token. If the token belongs to a Pro account, the rate limits and usage billing will correspond to Pro. Pro plan ($9/month) includes $2 of inference credit and higher base limits. After the free credits, you are billed per compute-second for usage beyond that (the pricing depends on model and hardware, which Hugging Face documents on their site). In contrast, a free token has no billing; it will simply stop working (429/402 errors) if you exceed free usage limits or if you attempt to use models marked as requiring payment.
It’s also possible to have a team or org token if you’re using organization assets – those would have their own limits (the 5-minute window is applied per user or org member, not globally shared, as noted in HF docs).
Model access: Note that some models on Hugging Face Hub are gated or private. Your token needs permission to access them (for example, if a model is only available to users who agreed to terms or to your organization). Public models like the sentence-transformers are accessible to all authenticated users.
In summary, for Hugging Face you will configure a model ID and an API token in our tool (Ploke). The token’s privileges (free or paid) will determine speed and quota, but the implementation in code (making a POST request with inputs) remains the same across tiers.
OpenAI Embeddings API
Request Format
OpenAI provides a specific endpoint for embeddings: POST https://api.openai.com/v1/embeddings. The request JSON includes two required fields: the model (which embedding model to use) and the input text to embed. Optionally, you can provide an array of texts as the input to get embeddings for a batch in one request. For example:
{
  "model": "text-embedding-ada-002",
  "input": ["This is a test sentence.", "And another sentence to embed."]
}
This will instruct the API to generate embeddings for the two input strings using the Ada-002 embedding model.
The OpenAI API expects an Authorization: Bearer YOUR_API_KEY header and Content-Type: application/json. Unlike Hugging Face, the model is specified in the payload, not in the URL. If the model field is omitted, the request will error – you must explicitly choose a model, as OpenAI has several embedding models available.
Batching: OpenAI’s "input" field can be a single string or an array of strings. Internally, the API will count the tokens of all inputs combined for billing, and return an embedding for each. Batching is often more efficient for throughput (it reduces HTTP overhead), though keep in mind the total token limit (the inputs concatenated cannot exceed the model’s context length, which is 8191 tokens for Ada-002 and the new v3 models).
Additional parameters: The embeddings endpoint is simpler than the completion or chat endpoints. The only other parameter OpenAI documents is an optional "user" identifier string you can send for tracking. This is not used to influence the embedding, but rather to help OpenAI with abuse detection (you might set it to an end-user ID in a multi-user application). There are no temperature, max_tokens, etc., in the embeddings API – it always returns a fixed-size vector.
As of late 2023, OpenAI introduced embedding v3 models that support a form of dimension reduction. However, this is not controlled via an API parameter but rather done client-side. Specifically, the new models produce embeddings where the latter part of the vector can be truncated with minimal loss of meaning (due to a technique called Multi-Rate Learning). For example, you might choose to use only the first 256 components of a 3-large embedding to save space. The API itself will return the full vector; truncation is up to the user.
Response Format and Embedding Dimensions
The OpenAI API will return a JSON object containing the embeddings and some metadata. An example response (for two inputs) looks like:
{
  "object": "list",
  "data": [
    {
      "object": "embedding",
      "embedding": [0.0023064255, -0.009327292, ... -0.0028842222],
      "index": 0
    },
    {
      "object": "embedding",
      "embedding": [0.0008123108,  0.015275321,  ... -0.007129852],
      "index": 1
    }
  ],
  "model": "text-embedding-ada-002",
  "usage": { "prompt_tokens": 32, "total_tokens": 32 }
}
Each entry in the data array is an embedding result with an "embedding" field (the vector of floats) and an "index" field indicating which input it corresponds to. The top-level "model" echoes which model was used, and "usage" reports how many tokens were counted in the request (useful for cost tracking).
Vector dimensionality: The length of the embedding vector depends on the model:
For text-embedding-ada-002 (the second-generation Ada model released Dec 2022), the vector has 1536 dimensions
newapi.ai
. This was until recently the primary embedding model used by OpenAI’s customers.
In January 2024, OpenAI introduced two third-generation embedding models:
text-embedding-3-small – which also produces 1536-dimensional embeddings (same size as Ada-002).
text-embedding-3-large – which produces 3072-dimensional embeddings.
Both of these have the same input limit (8191 tokens) and were trained on the same dataset cutoff (September 2021) as Ada-002. The small model is more efficient and actually outperforms Ada-002, especially on multilingual tasks, at a fraction of the cost. The large model is the new state-of-the-art in accuracy for OpenAI, at the cost of a bigger vector and higher compute usage.
Older models: OpenAI’s first-generation embedding models (from 2021) had various names like text-similarity-ada-001, text-similarity-babbage-001, up to text-search-davinci-001. These had smaller vector sizes (Ada-001 was 1024-d, Babbage ~2048-d, Curie ~4096-d, Davinci ~12288-d) and were primarily used for semantic search and similarity before Ada-002 arrived. In fact, Ada-002 was a leap that outperformed even the giant Davinci model while being much smaller. Those older models have been deprecated in favor of Ada-002. OpenAI has stated that Ada-002 will remain available and is not immediately deprecated by the new v3 models – so developers have a choice, though the new models are expected to replace Ada-002 in many applications going forward.
Encoding format: By default, OpenAI returns the embedding as an array of float64 values in JSON. This is human-readable but not the most compact. OpenAI’s API (and some third-party proxies) have an option to get embeddings in a compressed format (like base64-encoded). For instance, the OpenAI API accepts an encoding_format parameter with value "base64" to return the embedding as a base64 string instead of JSON array. However, if not needed, sticking to the default floats is simplest. In our implementation, we’ll parse the JSON into a vector of floats.
Rate Limits and Considerations
OpenAI’s API has rate limits that depend on your account and the model. These limits are typically measured in two dimensions: requests per minute (RPM) and tokens per minute (TPM). Every OpenAI API key has a default quota; if you need more, you can request an increase from OpenAI support. As of mid-2023, a new OpenAI user after entering billing info might have had limits like 3,000 RPM and 250,000 TPM for the embeddings endpoint using Ada-002. There was some documentation inconsistency where one page mentioned 3,500 RPM / 350k TPM for embeddings, but the user-specific dashboard showed 3,000 RPM / 1,000,000 TPM for Ada-002. In practice, OpenAI often sets very high token-per-minute limits for embeddings (since vectorizing large volumes of text is a common use), while the requests-per-minute may be the tighter constraint.
For the new embedding-3 models, OpenAI hasn’t publicly released specific rate numbers in the docs that we’ve seen. It’s likely similar or slightly adjusted. Typically, OpenAI might allocate a certain share of capacity per model. For instance, one might infer that text-embedding-3-large could have a lower RPM if it uses more compute. But as of now, the best approach is to check your account’s rate limit page on OpenAI’s dashboard (under Account > Rate limits) which will list the limits for each model.
Handling rate limits: If you exceed the limit, the API will respond with HTTP 429 Too Many Requests. The error body will usually contain a message like “Rate limit exceeded for requests per minute” or “You exceeded your current quota, please check your plan and billing details.”. OpenAI recommends exponential backoff on retries. In our tool, we might implement a simple retry with delay for such errors or at least propagate the error clearly. The user can also contact OpenAI to raise limits if they have a legitimate high-volume use case (especially common for embeddings where you might need to embed millions of texts for a vector database).
Free trial vs paid: OpenAI’s free trial users (those who have not added a credit card) have very limited hard caps (and as of 2025 OpenAI’s free credit is time-limited and quite small, e.g. $5 worth). To use the embeddings API reliably, one should have a paid account (even if you only incur a few cents of usage, the account needs to be enabled for paid usage after the free credit is exhausted). Once paid, the main limit is the rate limit as described. There is no concept of different tiers of service (aside from enterprise contracts) – everyone pays per token and gets roughly the same base rate limits, which scale with usage history and as granted by OpenAI.
Cost considerations: Embeddings are relatively inexpensive compared to text generation. For example, Ada-002 costs $0.0001 per 1K tokens. The new text-embedding-3-small is even cheaper at $0.00002 per 1K tokens (5x cheaper than Ada), and 3-large costs $0.00013 per 1K. This means you could embed 1 million tokens with 3-small for only $0.02, which is very affordable. These prices and performance differences might influence which model a user of our tool chooses (small vs large, etc.). We should expose the model choice and maybe provide guidance or defaults (perhaps default to the smaller model for cost efficiency, unless the user needs maximum accuracy).
Authentication and Access
API keys: OpenAI’s API uses a secret API key string (format starting with “sk-...”). The user must obtain this from their OpenAI dashboard and provide it to our application (Ploke) in a secure manner. Our implementation should accept this key (likely via a config file or environment variable) and include it in the Authorization header of each request. It’s important not to embed the key in any client-side code or logs, as it’s sensitive.
Project and organization IDs: In most cases, using the API key is enough. If the user is part of multiple organizations in OpenAI, the requests might be billed to their default org. The API also allows specifying an OpenAI-Organization header if needed, but for simplicity, we usually don’t need this unless the user explicitly wants to target a specific org (most users have just their personal account org).
No distinction between free and paid keys: The key itself doesn’t indicate whether it’s free trial or paid – that status is just on the account. So our code just needs to handle “invalid key” (401) errors and possibly instruct the user to check their billing if they get an error indicating they have no credit. Also, because the OpenAI key is required from the start (there’s no unauthenticated access), we’ll ensure the user provides it in the config.
Model availability: By default, all embedding models (Ada-002, 3-small, 3-large) are available to use with any API key that has access to the v1 API. OpenAI sometimes puts new models in a beta program that requires sign-up, but as of the embedding v3 launch, they made them generally available to all API users. We should still handle gracefully the case where a user tries to use a model name that doesn’t exist or isn’t available – the API would return an error which we can surface (e.g., “Model not found” or “The model is currently unavailable”). This is especially relevant if OpenAI adds more models in the future or deprecates old ones.
Designing a Generic Remote Embeddings Interface
To support multiple providers (initially Hugging Face and OpenAI) in our tool, we will create a flexible abstraction. The goal is to allow the same high-level operations (e.g., “embed this text”) to work with different underlying services by configuring which provider to use. Here’s how we can approach it:
Trait-based Abstraction: We can define a Rust trait (or an equivalent interface in the given language) like EmbeddingService that has a method for generating embeddings. For example:
trait EmbeddingService {
    fn embed(&self, texts: &[&str]) -> Result<Vec<Vec<f32>>, EmbedError>;
}
This trait returns a vector of embedding vectors for a slice of input strings. We choose a neutral return type (list of float vectors) that works for any provider. Each provider’s implementation will handle the specifics of calling that API and transforming the response into this form.
Provider Implementations: We then have structs that implement this trait for each provider, e.g. HuggingFaceEmbedder and OpenAIEmbedder. These will contain any configuration needed (API keys, endpoint URLs, model IDs, etc.). For instance:
struct HuggingFaceEmbedder {
    model_id: String,
    api_token: String,
    api_url: String  // could be constructed from model_id, e.g. "https://api-inference.huggingface.co/models/{model_id}"
}
struct OpenAIEmbedder {
    model: String,
    api_key: String,
    api_url: String  // probably always "https://api.openai.com/v1/embeddings"
}
Both could implement EmbeddingService. Internally, HuggingFaceEmbedder::embed would create the JSON {"inputs": texts} and POST it to self.api_url with the Bearer token. OpenAIEmbedder::embed would create {"model": self.model, "input": texts} and POST it with the API key auth. The trait hides these differences from the user of our library.
Handling Differences in Fields: As noted, Hugging Face expects the model in the URL and the inputs in the JSON, whereas OpenAI expects the model in JSON and has a fixed URL. Our code will need to handle these when constructing the request. We might design a common request builder internally that the implementations use. For example, a generic function that takes an endpoint, headers, and a JSON payload. The key is that the payload structure differs: HF payload is {"inputs": [...]} (plus optional "options"), OpenAI payload is {"model": ..., "input": [...]}. We’ll likely hardcode these formats in each implementation, or use a small enum to distinguish them.
Optional Parameters: We should ensure our abstraction doesn’t lose important capabilities. For HF, one may want to set options.wait_for_model=true or other pipeline options. For OpenAI, one might want to set the "user" field or adjust encoding_format. We can address this by allowing configuration of these in the struct. For example, our HuggingFaceEmbedder could have a flag for wait_for_model or accept a map of options. Our OpenAIEmbedder could have a field for user_id that, if set, adds that to the request. Another approach is to make the embed method itself take a config object or builder for advanced settings. Initially, we can start with the basics (cover 90% use cases) and then extend as needed.
Return Types: Both APIs ultimately give us an embedding vector or a list of vectors (for batch input). Hugging Face might return nested lists (e.g., if the model returns token-level embeddings), but in those cases we can decide to either flatten (perhaps by averaging the token embeddings) or to return an error/warning that the chosen model isn’t directly a single-vector model. In practice, if we document that the user should pick a sentence transformer model on HF for this purpose, we can assume the output is one vector per input. So, parsing:
HF: The response JSON for feature-extraction is either an array of arrays of floats (one per input) or an array of arrays of arrays (if token-level). We can detect that and handle it. For now, we might take the first sub-array if we encounter a 3D output or compute an average, but this could be a design decision.
OpenAI: The response JSON’s data field already gives one vector per input with an index. We will extract data[i].embedding for each i and return those. The order corresponds to the input order.
Error Handling: We should map HTTP errors to our EmbedError. For example, a 401 means auth is wrong (we can hint “check API token”), a 429 means rate limit (we could include that info). Both providers return some error structure: OpenAI uses an {"error": { "message": ..., "type": ..., "code": ... }} format, Hugging Face may return a simple error message or a 503 with model loading message. Our trait method should return a Result, and the error variant can carry either a provider-specific error or a unified error enum.
Extensibility: By designing around a trait, adding support for AWS, Azure, or others later would involve creating new structs implementing the same trait. For example, an AzureOpenAIEmbedder (very similar to OpenAI but different endpoint URL and API key format), or a CohereEmbedder (different API altogether). The caller (Ploke) could choose which implementation based on a configuration parameter like “provider = openai/hf”. This meets the “meet users where they are” goal – they can plug in their preferred embedding backend with minimal changes.
Configuration exposure: In Ploke’s config, we might allow something like:
[embeddings]
provider = "huggingface"  # or "openai"
model = "sentence-transformers/all-MiniLM-L6-v2"  # HF model ID
api_key = "hf_xxx"
or
[embeddings]
provider = "openai"
model = "text-embedding-ada-002"
api_key = "sk-xxx"
Our code would read this and instantiate the appropriate struct. We should indeed include older model options as valid values for model (for example, if someone wanted to use text-similarity-curie-001, they could try, even if it’s not recommended – OpenAI still allows it for now if one had access). On the HF side, “older” doesn’t apply as much in terms of API versioning, but it could mean older model checkpoints (which are just different model IDs).
In conclusion, we will document in our code and user guide how to configure either Hugging Face or OpenAI. The underlying trait implementation will ensure the correct JSON shape and fields are used for each. We have gathered the current details (as of Nov 13, 2025) for both APIs: the shapes of requests/responses, rate limits, vector sizes, and recommended practices, all of which will inform our implementation. Sources:
Omar E. “Getting Started With Embeddings.” HuggingFace Blog (2022) – explains Hugging Face embedding POST usage and options.
Hugging Face Documentation – Hub Rate Limits (Sep 2025) – rate limit tiers for free and pro users.
Hugging Face Forums – Inference API Rate Limits (May 2025) – user discussion confirming strict free tier limits and need for Pro for higher throughput.
HuggingFace Model Card: all-MiniLM-L6-v2 – notes 384-dimensional output vectors.
OpenAI API Reference – Embeddings endpoint (2023/2024) – response format and fields.
OpenAI Announcement – New embedding models and API updates (Jan 2024) – introduction of text-embedding-3-small/large, pricing and performance.
Pinecone Tech Blog – OpenAI’s Text Embeddings v3 (Jan 2024) – summary of embedding dimensions and benchmarks.
Reddit Q&A – clarification of OpenAI embedding rate limits (Sep 2023).
