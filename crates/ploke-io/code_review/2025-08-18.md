# ploke-io Code Review (2025-08-18)

Scope
- Reviewed files: src/lib.rs, README.md, Cargo.toml, full_plan.md, AI_NOTES.md, progress.md, test_coverage.md
- Focus: architecture, performance, correctness, API design, error handling, security, dependencies, tests, and documentation

Summary
ploke-io implements a clean actor-based, non-blocking I/O layer with a dedicated Tokio runtime, good separation via IoManagerHandle, and bounded concurrency using a semaphore informed by rlimit. The design is solid and test coverage is broad. However, several correctness and performance issues should be addressed: the hashing verification is fragile, process_file is too complex, result reordering is unnecessarily convoluted, and error handling contains inconsistencies with the shared ploke_error crate. Targeted refactors and a few policy decisions will improve robustness, performance, and maintainability.

Strengths
- Actor model with dedicated runtime and simple handle API
- Bounded concurrency with dynamic limit based on rlimit
- Token-based TrackingHash (syn token stream) aligns with functional changes
- Extensive tests across many edge cases, including shutdown and Unicode boundaries
- Good internal documentation and tracing hooks

Key Issues (prioritized)
1) Correctness: Per-file content verification relies on the first request’s file_tracking_hash
   - In process_file, actual_tracking_hash is compared only to requests[0].request.file_tracking_hash.
   - If batch inputs contain mixed/stale hashes for the same file, good requests can be incorrectly marked stale, or stale ones accepted if the first happens to match.
   - Action: Verify per-request against the single calculated actual_tracking_hash for that file. Emit ContentMismatch per-request as needed.

2) Complexity: process_file contains too many responsibilities and manual error plumbing
   - Reads file, decodes UTF-8, parses, generates hash, verifies, and extracts snippets with repeated error mapping.
   - Action: Extract helpers:
     - read_file_to_string(path) -> Result<String, IoError>
     - parse_tokens(content, path) -> Result<proc_macro2::TokenStream, IoError>
     - verify_file_hash(actual, expected_per_request) -> Option<IoError> per-request
     - extract_snippet(content, start, end, path) -> Result<String, IoError>
   - Use ? where possible to reduce branching and duplication.

3) Performance: Re-reading/parsing with no caching, sorting-based reordering
   - For heavy workloads, repeatedly parsing the same file is costly.
   - Action:
     - Consider an LRU cache for file content and/or TrackingHash keyed by (Path, mtime, size).
     - Pre-allocate results and fill by index instead of collect-then-sort.

4) Error handling: Inconsistent mapping and missing variant alignment
   - From<IoError> maps ShutdownInitiated to FatalError::ShutdownInitiated, but ploke-error may not define this variant (per full_plan.md). This risks compile/runtime mismatch across crates.
   - Action:
     - Align with workspace policy: either add FatalError::ShutdownInitiated in ploke-error, or map to InternalError::ShutdownInitiated (preferred per full_plan.md).
     - Reinstate From<RecvError> for IoError to keep the chain consistent.
     - Ensure ParseError stays Diagnostic-friendly; consider using InternalError for non-user-facing failures.

5) API/Data invariants: Hashing strategy responsibilities
   - IO actor currently verifies file-level TrackingHash derived from tokens. This couples I/O to AST parsing cost.
   - Action:
     - Decide policy: IO layer should ideally be byte-oriented and avoid parsing. If verification must stay, cache tokens/hash.
     - Alternatively, move verification responsibility to callers that already need AST context, and let IO return raw bytes safely (with range and UTF-8 checks).

Secondary Findings
- Result ordering logic: handle_read_snippet_batch collects Vec<(idx, Result<...>)>, sorts, and then reconstructs. Simpler to allocate Vec<Option<Result<...>>> of total_requests size and fill by index.
- Semaphore limit: Current logic uses min(100, soft/3) or 50 on error. Consider env override PLOKE_IO_FD_LIMIT with sane clamping and logging to aid ops.
- UTF-8 and boundaries: Good coverage and error messages. Keep enforcing char boundaries to avoid invalid UTF-8 slicing.
- Scan path: handle_scan_batch does not use concurrency_limit from semaphore.available_permits(); it spawns all and lets per-file check acquire a permit. Consider using a bounded concurrency stream (e.g., futures::stream::iter(...).buffer_unordered(N)) for better backpressure semantics.
- Security: Path validation and sanitization are not enforced. Consider rejecting relative paths and normalizing/canonicalizing. Avoid leaking absolute paths in user-facing messages unless necessary; keep detail in logs.
- Dependencies:
  - tokio-stream is listed but not used (src/lib.rs uses futures::stream). Remove if unused.
  - seahash only appears in README example; consider removing from dev-dependencies or moving the example behind a feature/doc cfg.
  - Optionally add memmap2 and lru as optional features for future optimizations.
- Documentation:
  - README shows seahash-based hashing sample which is inconsistent with TrackingHash usage. Update to reflect canonical hashing strategy or remove hash snippet entirely.
  - Add docs for OrderedRequest and IoManagerMessage as noted in full_plan.md.
- Tests:
  - test_get_snippets_batch_preserves_order is ignored; unblock when NodeId/file-level policy is decided.
  - Add tests for: runtime init failure (builder failure injection), semaphore exhaustion/acquisition failure (simulate via small limit), malformed UUIDs, and a large file (>1MB) path.
  - Add parse-failure test with syntactically invalid Rust.

Recommendations and Plan
Quick Wins (1–3 days)
- Verify per-request hash in process_file using the single computed actual_tracking_hash for that file.
- Simplify result reordering with a pre-allocated Vec<Option<Result<...>>> filled by idx.
- Reinstate From<RecvError> for IoError and decide on mapping to Internal error type or add ShutdownInitiated to ploke-error.
- Remove tokio-stream if unused; reconcile README hashing example with TrackingHash approach.
- Add env override for semaphore limit: PLOKE_IO_FD_LIMIT with parse fallback and clamped bounds.

Short Term (1–2 weeks)
- Refactor process_file into helpers; reduce early returns and cloned error payloads.
- Implement bounded concurrency for scan path via buffer_unordered with a limit equal to semaphore permits.
- Add tests:
  - Large file read (>1MB)
  - Parse error scenario
  - Semaphore exhaustion with very low limit
  - Shutdown with no active operations
  - Runtime initialization failures (if feasible to simulate)
- Add path validation checks (reject relative, .. components; optionally canonicalize).

Medium Term (2–4 weeks)
- Introduce an optional content/hash cache (LRU) keyed by (Path, mtime, size) to eliminate re-reading/parsing.
- Consider deferring AST parsing to the caller; make IO actor strictly byte-oriented plus UTF-8-safe slicing.
- Add feature flags: caching, memmap2 for large files, and remove sample seahash code entirely from docs.

Long Term
- Add watcher integration (notify) for proactive invalidation of caches and live updates.
- Evaluate write operations pipeline once read-path is stable and well-tested.

Illustrative Changes (pseudocode)

- Per-request verification and simpler ordering
```rust
// After computing actual_tracking_hash for file_path:
let mut final_results: Vec<Option<Result<String, PlokeError>>> = vec![None; total_requests];

for req in requests {
    if req.request.file_tracking_hash != actual_tracking_hash {
        final_results[req.idx] = Some(Err(IoError::ContentMismatch {
            path: file_path.clone(),
            name: req.request.name.clone(),
            id: req.request.id,
            file_tracking_hash: req.request.file_tracking_hash.0,
            namespace,
        }.into()));
        continue;
    }

    final_results[req.idx] = Some(extract_snippet(&file_content, req.request.start_byte, req.request.end_byte, &file_path));
}

let results = final_results.into_iter().map(|r| r.unwrap_or_else(|| Err(ploke_error::InternalError::InvalidState("Result missing for request").into()))).collect();
```

- From<RecvError> for IoError (restore the consistent chain)
```rust
impl From<RecvError> for IoError {
    fn from(e: RecvError) -> Self {
        IoError::Recv(e)
    }
}
```

- Env-based semaphore limit override with clamp
```rust
let default = match rlimit::getrlimit(rlimit::Resource::NOFILE) {
    Ok((soft, _)) => std::cmp::min(100, (soft / 3) as usize),
    Err(_) => 50,
};
let limit = std::env::var("PLOKE_IO_FD_LIMIT")
    .ok()
    .and_then(|s| s.parse::<usize>().ok())
    .map(|n| n.clamp(4, 1024))
    .unwrap_or(default);
```

Risk Assessment
- Changing error variant mapping (ShutdownInitiated) requires workspace-wide agreement.
- Adding caches introduces invalidation complexity; start with size/mtime keys and document invariants.
- Switching to byte-only IO actor may shift responsibilities to callers; plan migration carefully.

Closing Notes
The crate is in a good state to harden. Fixing the per-request verification and simplifying process_file will pay immediate dividends in correctness and readability. Caching and API boundary clarification will address performance and long-term maintainability. The testing plan is solid; prioritize the remaining “still needs test” items called out in test_coverage.md.

References
- full_plan.md: Problem list and recommendations
- AI_NOTES.md: Architectural analysis and proposed improvements
- progress.md: Recent changes log
- test_coverage.md: Gaps and checklist
- src/lib.rs: Core actor and request handling
- Cargo.toml: Dependencies and potential cleanup
