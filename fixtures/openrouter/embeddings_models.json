{
  "data": [
    {
      "id": "thenlper/gte-base",
      "name": "Thenlper: GTE-Base",
      "created": 1763433820,
      "description": "The gte-base embedding model encodes English sentences and paragraphs into a 768-dimensional dense vector space, delivering efficient and effective semantic embeddings optimized for textual similarity, semantic search, and clustering applications.",
      "architecture": {
        "input_modalities": [
          "text"
        ],
        "modality": "text->embeddings",
        "output_modalities": [
          "embeddings"
        ],
        "tokenizer": "Other"
      },
      "top_provider": {
        "is_moderated": false,
        "context_length": 512,
        "max_completion_tokens": null
      },
      "pricing": {
        "prompt": 5e-9,
        "completion": 0.0,
        "image": 0.0,
        "internal_reasoning": 0.0,
        "request": 0.0,
        "web_search": 0.0
      },
      "canonical_slug": "thenlper/gte-base-20251117",
      "context_length": 512,
      "hugging_face_id": "thenlper/gte-base",
      "per_request_limits": null,
      "supported_parameters": [
        "frequency_penalty",
        "max_tokens",
        "min_p",
        "presence_penalty",
        "repetition_penalty",
        "response_format",
        "seed",
        "stop",
        "temperature",
        "top_k",
        "top_p"
      ]
    },
    {
      "id": "thenlper/gte-large",
      "name": "Thenlper: GTE-Large",
      "created": 1763433655,
      "description": "The gte-large embedding model converts English sentences, paragraphs and moderate-length documents into a 1024-dimensional dense vector space, delivering high-quality semantic embeddings optimized for information retrieval, semantic textual similarity, reranking and clustering tasks. Trained via multi-stage contrastive learning on a large domain-diverse relevance corpus, it offers excellent performance across general-purpose embedding use-cases.",
      "architecture": {
        "input_modalities": [
          "text"
        ],
        "modality": "text->embeddings",
        "output_modalities": [
          "embeddings"
        ],
        "tokenizer": "Other"
      },
      "top_provider": {
        "is_moderated": false,
        "context_length": 512,
        "max_completion_tokens": null
      },
      "pricing": {
        "prompt": 1e-8,
        "completion": 0.0,
        "image": 0.0,
        "internal_reasoning": 0.0,
        "request": 0.0,
        "web_search": 0.0
      },
      "canonical_slug": "thenlper/gte-large-20251117",
      "context_length": 512,
      "hugging_face_id": "thenlper/gte-large",
      "per_request_limits": null,
      "supported_parameters": [
        "frequency_penalty",
        "max_tokens",
        "min_p",
        "presence_penalty",
        "repetition_penalty",
        "response_format",
        "seed",
        "stop",
        "temperature",
        "top_k",
        "top_p"
      ]
    },
    {
      "id": "intfloat/e5-large-v2",
      "name": "Intfloat: E5-Large-v2",
      "created": 1763433432,
      "description": "The e5-large-v2 embedding model maps English sentences, paragraphs, and documents into a 1024-dimensional dense vector space, delivering high-accuracy semantic embeddings optimized for retrieval, semantic search, reranking, and similarity-scoring tasks.",
      "architecture": {
        "input_modalities": [
          "text"
        ],
        "modality": "text->embeddings",
        "output_modalities": [
          "embeddings"
        ],
        "tokenizer": "Other"
      },
      "top_provider": {
        "is_moderated": false,
        "context_length": 512,
        "max_completion_tokens": null
      },
      "pricing": {
        "prompt": 1e-8,
        "completion": 0.0,
        "image": 0.0,
        "internal_reasoning": 0.0,
        "request": 0.0,
        "web_search": 0.0
      },
      "canonical_slug": "intfloat/e5-large-v2-20251117",
      "context_length": 512,
      "hugging_face_id": "intfloat/e5-large-v2",
      "per_request_limits": null,
      "supported_parameters": [
        "frequency_penalty",
        "max_tokens",
        "min_p",
        "presence_penalty",
        "repetition_penalty",
        "response_format",
        "seed",
        "stop",
        "temperature",
        "top_k",
        "top_p"
      ]
    },
    {
      "id": "intfloat/e5-base-v2",
      "name": "Intfloat: E5-Base-v2",
      "created": 1763433192,
      "description": "The e5-base-v2 embedding model encodes English sentences and paragraphs into a 768-dimensional dense vector space, producing efficient and high-quality semantic embeddings optimized for tasks such as semantic search, similarity scoring, retrieval and clustering.",
      "architecture": {
        "input_modalities": [
          "text"
        ],
        "modality": "text->embeddings",
        "output_modalities": [
          "embeddings"
        ],
        "tokenizer": "Other"
      },
      "top_provider": {
        "is_moderated": false,
        "context_length": 512,
        "max_completion_tokens": null
      },
      "pricing": {
        "prompt": 5e-9,
        "completion": 0.0,
        "image": 0.0,
        "internal_reasoning": 0.0,
        "request": 0.0,
        "web_search": 0.0
      },
      "canonical_slug": "intfloat/e5-base-v2-20251117",
      "context_length": 512,
      "hugging_face_id": "intfloat/e5-base-v2",
      "per_request_limits": null,
      "supported_parameters": [
        "frequency_penalty",
        "max_tokens",
        "min_p",
        "presence_penalty",
        "repetition_penalty",
        "response_format",
        "seed",
        "stop",
        "temperature",
        "top_k",
        "top_p"
      ]
    },
    {
      "id": "intfloat/multilingual-e5-large",
      "name": "Intfloat: Multilingual-E5-Large",
      "created": 1763433047,
      "description": "The multilingual-e5-large embedding model encodes sentences, paragraphs, and documents across over 90 languages into a 1024-dimensional dense vector space, delivering robust semantic embeddings optimized for multilingual retrieval, cross-language similarity, and large-scale data search.",
      "architecture": {
        "input_modalities": [
          "text"
        ],
        "modality": "text->embeddings",
        "output_modalities": [
          "embeddings"
        ],
        "tokenizer": "Other"
      },
      "top_provider": {
        "is_moderated": false,
        "context_length": 512,
        "max_completion_tokens": null
      },
      "pricing": {
        "prompt": 1e-8,
        "completion": 0.0,
        "image": 0.0,
        "internal_reasoning": 0.0,
        "request": 0.0,
        "web_search": 0.0
      },
      "canonical_slug": "intfloat/multilingual-e5-large-20251117",
      "context_length": 512,
      "hugging_face_id": "intfloat/multilingual-e5-large",
      "per_request_limits": null,
      "supported_parameters": [
        "frequency_penalty",
        "max_tokens",
        "min_p",
        "presence_penalty",
        "repetition_penalty",
        "response_format",
        "seed",
        "stop",
        "temperature",
        "top_k",
        "top_p"
      ]
    },
    {
      "id": "sentence-transformers/paraphrase-minilm-l6-v2",
      "name": "Sentence Transformers: paraphrase-MiniLM-L6-v2",
      "created": 1763432454,
      "description": "The paraphrase-MiniLM-L6-v2 embedding model converts sentences and short paragraphs into a 384-dimensional dense vector space, producing high-quality semantic embeddings optimized for paraphrase detection, semantic similarity scoring, clustering, and lightweight retrieval tasks.",
      "architecture": {
        "input_modalities": [
          "text"
        ],
        "modality": "text->embeddings",
        "output_modalities": [
          "embeddings"
        ],
        "tokenizer": "Other"
      },
      "top_provider": {
        "is_moderated": false,
        "context_length": 512,
        "max_completion_tokens": null
      },
      "pricing": {
        "prompt": 5e-9,
        "completion": 0.0,
        "image": 0.0,
        "internal_reasoning": 0.0,
        "request": 0.0,
        "web_search": 0.0
      },
      "canonical_slug": "sentence-transformers/paraphrase-minilm-l6-v2-20251117",
      "context_length": 512,
      "hugging_face_id": "sentence-transformers/paraphrase-MiniLM-L6-v2",
      "per_request_limits": null,
      "supported_parameters": [
        "frequency_penalty",
        "max_tokens",
        "min_p",
        "presence_penalty",
        "repetition_penalty",
        "response_format",
        "seed",
        "stop",
        "temperature",
        "top_k",
        "top_p"
      ]
    },
    {
      "id": "sentence-transformers/all-minilm-l12-v2",
      "name": "Sentence Transformers: all-MiniLM-L12-v2",
      "created": 1763432155,
      "description": "The all-MiniLM-L12-v2 embedding model maps sentences and short paragraphs into a 384-dimensional dense vector space, producing efficient and high-quality semantic embeddings optimized for tasks such as semantic search, clustering, and similarity-scoring.",
      "architecture": {
        "input_modalities": [
          "text"
        ],
        "modality": "text->embeddings",
        "output_modalities": [
          "embeddings"
        ],
        "tokenizer": "Other"
      },
      "top_provider": {
        "is_moderated": false,
        "context_length": 512,
        "max_completion_tokens": null
      },
      "pricing": {
        "prompt": 5e-9,
        "completion": 0.0,
        "image": 0.0,
        "internal_reasoning": 0.0,
        "request": 0.0,
        "web_search": 0.0
      },
      "canonical_slug": "sentence-transformers/all-minilm-l12-v2-20251117",
      "context_length": 512,
      "hugging_face_id": "sentence-transformers/all-MiniLM-L12-v2",
      "per_request_limits": null,
      "supported_parameters": [
        "frequency_penalty",
        "max_tokens",
        "min_p",
        "presence_penalty",
        "repetition_penalty",
        "response_format",
        "seed",
        "stop",
        "temperature",
        "top_k",
        "top_p"
      ]
    },
    {
      "id": "baai/bge-base-en-v1.5",
      "name": "BAAI: bge-base-en-v1.5",
      "created": 1763431837,
      "description": "The bge-base-en-v1.5 embedding model converts English sentences and paragraphs into 768-dimensional dense vectors, delivering efficient, high-quality semantic embeddings optimized for retrieval, semantic search, and document-matching workflows. This version (v1.5) features improved similarity-score distribution and stronger retrieval performance out of the box.",
      "architecture": {
        "input_modalities": [
          "text"
        ],
        "modality": "text->embeddings",
        "output_modalities": [
          "embeddings"
        ],
        "tokenizer": "Other"
      },
      "top_provider": {
        "is_moderated": false,
        "context_length": 512,
        "max_completion_tokens": null
      },
      "pricing": {
        "prompt": 5e-9,
        "completion": 0.0,
        "image": 0.0,
        "internal_reasoning": 0.0,
        "request": 0.0,
        "web_search": 0.0
      },
      "canonical_slug": "baai/bge-base-en-v1.5-20251117",
      "context_length": 512,
      "hugging_face_id": "BAAI/bge-base-en-v1.5",
      "per_request_limits": null,
      "supported_parameters": [
        "frequency_penalty",
        "max_tokens",
        "min_p",
        "presence_penalty",
        "repetition_penalty",
        "response_format",
        "seed",
        "stop",
        "temperature",
        "top_k",
        "top_p"
      ]
    },
    {
      "id": "sentence-transformers/multi-qa-mpnet-base-dot-v1",
      "name": "Sentence Transformers: multi-qa-mpnet-base-dot-v1",
      "created": 1763431339,
      "description": "The multi-qa-mpnet-base-dot-v1 embedding model transforms sentences and short paragraphs into a 768-dimensional dense vector space, generating high-quality semantic embeddings optimized for question-and-answer retrieval, semantic search, and similarity-scoring across diverse content.",
      "architecture": {
        "input_modalities": [
          "text"
        ],
        "modality": "text->embeddings",
        "output_modalities": [
          "embeddings"
        ],
        "tokenizer": "Other"
      },
      "top_provider": {
        "is_moderated": false,
        "context_length": 512,
        "max_completion_tokens": null
      },
      "pricing": {
        "prompt": 5e-9,
        "completion": 0.0,
        "image": 0.0,
        "internal_reasoning": 0.0,
        "request": 0.0,
        "web_search": 0.0
      },
      "canonical_slug": "sentence-transformers/multi-qa-mpnet-base-dot-v1-20251117",
      "context_length": 512,
      "hugging_face_id": "sentence-transformers/multi-qa-mpnet-base-dot-v1",
      "per_request_limits": null,
      "supported_parameters": [
        "frequency_penalty",
        "max_tokens",
        "min_p",
        "presence_penalty",
        "repetition_penalty",
        "response_format",
        "seed",
        "stop",
        "temperature",
        "top_k",
        "top_p"
      ]
    },
    {
      "id": "baai/bge-large-en-v1.5",
      "name": "BAAI: bge-large-en-v1.5",
      "created": 1763431087,
      "description": "The bge-large-en-v1.5 embedding model maps English sentences, paragraphs, and documents into a 1024-dimensional dense vector space, delivering high-fidelity semantic embeddings optimized for semantic search, document retrieval, and downstream NLP tasks in English.",
      "architecture": {
        "input_modalities": [
          "text"
        ],
        "modality": "text->embeddings",
        "output_modalities": [
          "embeddings"
        ],
        "tokenizer": "Other"
      },
      "top_provider": {
        "is_moderated": false,
        "context_length": 512,
        "max_completion_tokens": null
      },
      "pricing": {
        "prompt": 1e-8,
        "completion": 0.0,
        "image": 0.0,
        "internal_reasoning": 0.0,
        "request": 0.0,
        "web_search": 0.0
      },
      "canonical_slug": "baai/bge-large-en-v1.5-20251117",
      "context_length": 512,
      "hugging_face_id": "BAAI/bge-large-en-v1.5",
      "per_request_limits": null,
      "supported_parameters": [
        "frequency_penalty",
        "max_tokens",
        "min_p",
        "presence_penalty",
        "repetition_penalty",
        "response_format",
        "seed",
        "stop",
        "temperature",
        "top_k",
        "top_p"
      ]
    },
    {
      "id": "baai/bge-m3",
      "name": "BAAI: bge-m3",
      "created": 1763424372,
      "description": "The bge-m3 embedding model encodes sentences, paragraphs, and long documents into a 1024-dimensional dense vector space, delivering high-quality semantic embeddings optimized for multilingual retrieval, semantic search, and large-context applications.",
      "architecture": {
        "input_modalities": [
          "text"
        ],
        "modality": "text->embeddings",
        "output_modalities": [
          "embeddings"
        ],
        "tokenizer": "Other"
      },
      "top_provider": {
        "is_moderated": false,
        "context_length": 8192,
        "max_completion_tokens": null
      },
      "pricing": {
        "prompt": 1e-8,
        "completion": 0.0,
        "image": 0.0,
        "internal_reasoning": 0.0,
        "request": 0.0,
        "web_search": 0.0
      },
      "canonical_slug": "baai/bge-m3-20251117",
      "context_length": 8192,
      "hugging_face_id": "BAAI/bge-m3",
      "per_request_limits": null,
      "supported_parameters": [
        "frequency_penalty",
        "max_tokens",
        "min_p",
        "presence_penalty",
        "repetition_penalty",
        "response_format",
        "seed",
        "stop",
        "temperature",
        "top_k",
        "top_p"
      ]
    },
    {
      "id": "sentence-transformers/all-mpnet-base-v2",
      "name": "Sentence Transformers: all-mpnet-base-v2",
      "created": 1763421830,
      "description": "The all-mpnet-base-v2 embedding model encodes sentences and short paragraphs into a 768-dimensional dense vector space, providing high-fidelity semantic embeddings well suited for tasks like information retrieval, clustering, similarity scoring, and text ranking.",
      "architecture": {
        "input_modalities": [
          "text"
        ],
        "modality": "text->embeddings",
        "output_modalities": [
          "embeddings"
        ],
        "tokenizer": "Other"
      },
      "top_provider": {
        "is_moderated": false,
        "context_length": 512,
        "max_completion_tokens": null
      },
      "pricing": {
        "prompt": 5e-9,
        "completion": 0.0,
        "image": 0.0,
        "internal_reasoning": 0.0,
        "request": 0.0,
        "web_search": 0.0
      },
      "canonical_slug": "sentence-transformers/all-mpnet-base-v2-20251117",
      "context_length": 512,
      "hugging_face_id": "sentence-transformers/all-mpnet-base-v2",
      "per_request_limits": null,
      "supported_parameters": [
        "frequency_penalty",
        "max_tokens",
        "min_p",
        "presence_penalty",
        "repetition_penalty",
        "response_format",
        "seed",
        "stop",
        "temperature",
        "top_k",
        "top_p"
      ]
    },
    {
      "id": "sentence-transformers/all-minilm-l6-v2",
      "name": "Sentence Transformers: all-MiniLM-L6-v2",
      "created": 1763421176,
      "description": "The all-MiniLM-L6-v2 embedding model maps sentences and short paragraphs into a 384-dimensional dense vector space, enabling high-quality semantic representations that are ideal for downstream tasks such as information retrieval, clustering, similarity scoring, and text ranking.",
      "architecture": {
        "input_modalities": [
          "text"
        ],
        "modality": "text->embeddings",
        "output_modalities": [
          "embeddings"
        ],
        "tokenizer": "Other"
      },
      "top_provider": {
        "is_moderated": false,
        "context_length": 512,
        "max_completion_tokens": null
      },
      "pricing": {
        "prompt": 5e-9,
        "completion": 0.0,
        "image": 0.0,
        "internal_reasoning": 0.0,
        "request": 0.0,
        "web_search": 0.0
      },
      "canonical_slug": "sentence-transformers/all-minilm-l6-v2-20251117",
      "context_length": 512,
      "hugging_face_id": "sentence-transformers/all-MiniLM-L6-v2",
      "per_request_limits": null,
      "supported_parameters": [
        "frequency_penalty",
        "max_tokens",
        "min_p",
        "presence_penalty",
        "repetition_penalty",
        "response_format",
        "seed",
        "stop",
        "temperature",
        "top_k",
        "top_p"
      ]
    },
    {
      "id": "mistralai/mistral-embed-2312",
      "name": "Mistral: Mistral Embed 2312",
      "created": 1761944622,
      "description": "Mistral Embed is a specialized embedding model for text data, optimized for semantic search and RAG applications. Developed by Mistral AI in late 2023, it produces 1024-dimensional vectors that effectively capture semantic relationships in text.",
      "architecture": {
        "input_modalities": [
          "text"
        ],
        "modality": "text->embeddings",
        "output_modalities": [
          "embeddings"
        ],
        "tokenizer": "Mistral"
      },
      "top_provider": {
        "is_moderated": false,
        "context_length": 8192,
        "max_completion_tokens": null
      },
      "pricing": {
        "prompt": 1e-7,
        "completion": 0.0,
        "image": 0.0,
        "internal_reasoning": 0.0,
        "request": 0.0,
        "web_search": 0.0
      },
      "canonical_slug": "mistralai/mistral-embed-2312",
      "context_length": 8192,
      "hugging_face_id": null,
      "per_request_limits": null,
      "supported_parameters": [
        "frequency_penalty",
        "max_tokens",
        "presence_penalty",
        "response_format",
        "seed",
        "stop",
        "structured_outputs",
        "temperature",
        "top_p"
      ]
    },
    {
      "id": "google/gemini-embedding-001",
      "name": "Google: Gemini Embedding 001",
      "created": 1761943410,
      "description": "gemini-embedding-001 provides a unified cutting edge experience across domains, including science, legal, finance, and coding. This embedding model has consistently held a top spot on the Massive Text Embedding Benchmark (MTEB) Multilingual leaderboard since the experimental launch in March.",
      "architecture": {
        "input_modalities": [
          "text"
        ],
        "modality": "text->embeddings",
        "output_modalities": [
          "embeddings"
        ],
        "tokenizer": "Gemini"
      },
      "top_provider": {
        "is_moderated": false,
        "context_length": 20000,
        "max_completion_tokens": null
      },
      "pricing": {
        "prompt": 1.5e-7,
        "completion": 0.0,
        "image": 0.0,
        "internal_reasoning": 0.0,
        "request": 0.0,
        "web_search": 0.0
      },
      "canonical_slug": "google/gemini-embedding-001",
      "context_length": 20000,
      "hugging_face_id": "",
      "per_request_limits": null,
      "supported_parameters": [
        "max_tokens",
        "response_format",
        "seed",
        "structured_outputs",
        "temperature",
        "top_p"
      ]
    },
    {
      "id": "openai/text-embedding-ada-002",
      "name": "OpenAI: Text Embedding Ada 002",
      "created": 1761865798,
      "description": "text-embedding-ada-002 is OpenAI's legacy text embedding model.",
      "architecture": {
        "input_modalities": [
          "text"
        ],
        "modality": "text->embeddings",
        "output_modalities": [
          "embeddings"
        ],
        "tokenizer": "Other"
      },
      "top_provider": {
        "is_moderated": true,
        "context_length": 8192,
        "max_completion_tokens": null
      },
      "pricing": {
        "prompt": 1e-7,
        "completion": 0.0,
        "image": 0.0,
        "internal_reasoning": 0.0,
        "request": 0.0,
        "web_search": 0.0
      },
      "canonical_slug": "openai/text-embedding-ada-002",
      "context_length": 8192,
      "hugging_face_id": "",
      "per_request_limits": null,
      "supported_parameters": [
        "frequency_penalty",
        "logit_bias",
        "logprobs",
        "max_tokens",
        "presence_penalty",
        "response_format",
        "seed",
        "stop",
        "structured_outputs",
        "temperature",
        "top_logprobs",
        "top_p"
      ]
    },
    {
      "id": "mistralai/codestral-embed-2505",
      "name": "Mistral: Codestral Embed 2505",
      "created": 1761864460,
      "description": "Mistral Codestral Embed is specially designed for code, perfect for embedding code databases, repositories, and powering coding assistants with state-of-the-art retrieval.",
      "architecture": {
        "input_modalities": [
          "text"
        ],
        "modality": "text->embeddings",
        "output_modalities": [
          "embeddings"
        ],
        "tokenizer": "Mistral"
      },
      "top_provider": {
        "is_moderated": false,
        "context_length": 8192,
        "max_completion_tokens": null
      },
      "pricing": {
        "prompt": 1.5e-7,
        "completion": 0.0,
        "image": 0.0,
        "internal_reasoning": 0.0,
        "request": 0.0,
        "web_search": 0.0
      },
      "canonical_slug": "mistralai/codestral-embed-2505",
      "context_length": 8192,
      "hugging_face_id": "",
      "per_request_limits": null,
      "supported_parameters": [
        "frequency_penalty",
        "max_tokens",
        "presence_penalty",
        "response_format",
        "seed",
        "stop",
        "structured_outputs",
        "temperature",
        "top_p"
      ]
    },
    {
      "id": "openai/text-embedding-3-large",
      "name": "OpenAI: Text Embedding 3 Large",
      "created": 1761862866,
      "description": "text-embedding-3-large is OpenAI's most capable embedding model for both english and non-english tasks. Embeddings are a numerical representation of text that can be used to measure the relatedness between two pieces of text. Embeddings are useful for search, clustering, recommendations, anomaly detection, and classification tasks.",
      "architecture": {
        "input_modalities": [
          "text"
        ],
        "modality": "text->embeddings",
        "output_modalities": [
          "embeddings"
        ],
        "tokenizer": "Other"
      },
      "top_provider": {
        "is_moderated": true,
        "context_length": 8192,
        "max_completion_tokens": null
      },
      "pricing": {
        "prompt": 1.3e-7,
        "completion": 0.0,
        "image": 0.0,
        "internal_reasoning": 0.0,
        "request": 0.0,
        "web_search": 0.0
      },
      "canonical_slug": "openai/text-embedding-3-large",
      "context_length": 8192,
      "hugging_face_id": "",
      "per_request_limits": null,
      "supported_parameters": [
        "frequency_penalty",
        "logit_bias",
        "logprobs",
        "max_tokens",
        "presence_penalty",
        "response_format",
        "seed",
        "stop",
        "structured_outputs",
        "temperature",
        "top_logprobs",
        "top_p"
      ]
    },
    {
      "id": "openai/text-embedding-3-small",
      "name": "OpenAI: Text Embedding 3 Small",
      "created": 1761857455,
      "description": " text-embedding-3-small is OpenAI's improved, more performant version of the ada embedding model. Embeddings are a numerical representation of text that can be used to measure the relatedness between two pieces of text. Embeddings are useful for search, clustering, recommendations, anomaly detection, and classification tasks.",
      "architecture": {
        "input_modalities": [
          "text"
        ],
        "modality": "text->embeddings",
        "output_modalities": [
          "embeddings"
        ],
        "tokenizer": "Other"
      },
      "top_provider": {
        "is_moderated": true,
        "context_length": 8192,
        "max_completion_tokens": null
      },
      "pricing": {
        "prompt": 2e-8,
        "completion": 0.0,
        "image": 0.0,
        "internal_reasoning": 0.0,
        "request": 0.0,
        "web_search": 0.0
      },
      "canonical_slug": "openai/text-embedding-3-small",
      "context_length": 8192,
      "hugging_face_id": "",
      "per_request_limits": null,
      "supported_parameters": [
        "frequency_penalty",
        "logit_bias",
        "logprobs",
        "max_tokens",
        "presence_penalty",
        "response_format",
        "seed",
        "stop",
        "structured_outputs",
        "temperature",
        "top_logprobs",
        "top_p"
      ]
    },
    {
      "id": "qwen/qwen3-embedding-8b",
      "name": "Qwen: Qwen3 Embedding 8B",
      "created": 1761680622,
      "description": "The Qwen3 Embedding model series is the latest proprietary model of the Qwen family, specifically designed for text embedding and ranking tasks. This series inherits the exceptional multilingual capabilities, long-text understanding, and reasoning skills of its foundational model. The Qwen3 Embedding series represents significant advancements in multiple text embedding and ranking tasks, including text retrieval, code retrieval, text classification, text clustering, and bitext mining.",
      "architecture": {
        "input_modalities": [
          "text"
        ],
        "modality": "text->embeddings",
        "output_modalities": [
          "embeddings"
        ],
        "tokenizer": "Other"
      },
      "top_provider": {
        "is_moderated": false,
        "context_length": 32768,
        "max_completion_tokens": null
      },
      "pricing": {
        "prompt": 1e-8,
        "completion": 0.0,
        "image": 0.0,
        "internal_reasoning": 0.0,
        "request": 0.0,
        "web_search": 0.0
      },
      "canonical_slug": "qwen/qwen3-embedding-8b",
      "context_length": 32768,
      "hugging_face_id": "Qwen/Qwen3-Embedding-8B",
      "per_request_limits": null,
      "supported_parameters": [
        "frequency_penalty",
        "logit_bias",
        "logprobs",
        "max_tokens",
        "min_p",
        "presence_penalty",
        "repetition_penalty",
        "response_format",
        "seed",
        "stop",
        "temperature",
        "top_k",
        "top_logprobs",
        "top_p"
      ]
    },
    {
      "id": "qwen/qwen3-embedding-4b",
      "name": "Qwen: Qwen3 Embedding 4B",
      "created": 1761662922,
      "description": "The Qwen3 Embedding model series is the latest proprietary model of the Qwen family, specifically designed for text embedding and ranking tasks. This series inherits the exceptional multilingual capabilities, long-text understanding, and reasoning skills of its foundational model. The Qwen3 Embedding series represents significant advancements in multiple text embedding and ranking tasks, including text retrieval, code retrieval, text classification, text clustering, and bitext mining.",
      "architecture": {
        "input_modalities": [
          "text"
        ],
        "modality": "text->embeddings",
        "output_modalities": [
          "embeddings"
        ],
        "tokenizer": "Other"
      },
      "top_provider": {
        "is_moderated": false,
        "context_length": 32768,
        "max_completion_tokens": null
      },
      "pricing": {
        "prompt": 2e-8,
        "completion": 0.0,
        "image": 0.0,
        "internal_reasoning": 0.0,
        "request": 0.0,
        "web_search": 0.0
      },
      "canonical_slug": "qwen/qwen3-embedding-4b",
      "context_length": 32768,
      "hugging_face_id": "Qwen/Qwen3-Embedding-4B",
      "per_request_limits": null,
      "supported_parameters": [
        "frequency_penalty",
        "max_tokens",
        "min_p",
        "presence_penalty",
        "repetition_penalty",
        "response_format",
        "seed",
        "stop",
        "temperature",
        "top_k",
        "top_p"
      ]
    }
  ]
}