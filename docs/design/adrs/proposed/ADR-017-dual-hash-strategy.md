# ADR-017: Dual-Hash Strategy for Semantic and File Integrity

## Status
PROPOSED

## Context
The system needs a way to verify that the content of a file on disk has not changed since it was last indexed. This is crucial for the `ploke-io` actor, which reads file snippets based on byte offsets provided by the database. If the file has changed, these offsets may be incorrect, leading to corrupted data.

At the same time, the system needs to detect *semantic* changes to code items (functions, structs, etc.) to know when to trigger expensive re-indexing operations like generating new vector embeddings. A simple byte-level hash is too sensitive for this, as it would be triggered by trivial changes like adding a newline or a comment.

The initial approach considered using a single hash (`TrackingHash`, based on a `TokenStream`) for both purposes. However, this is not viable because the I/O layer does not have access to the token stream and can only verify raw bytes, which would never match a token-based hash.

## Decision
We will adopt a dual-hash strategy that uses two distinct types of hashes for two different purposes:

1.  **`FileContentHash`**: A fast, byte-level hash (e.g., using `blake3`) of a file's raw content.
    *   **Purpose**: To provide a high-performance, memory-efficient way for the `ploke-io` actor to verify that a file's content on disk is identical to the version that was indexed.
    *   **Generation**: Generated by the parser (`syn_parser`) once per file during the initial indexing phase.
    *   **Storage**: Stored in the database, associated with the file node.

2.  **`TrackingHash`**: A semantic, token-based hash (a `Uuid` derived from a `TokenStream`).
    *   **Purpose**: To detect meaningful changes to a code item, ignoring superficial differences like whitespace or comments. This hash is used to determine when to re-run expensive semantic analysis (e.g., embedding generation).
    *   **Generation**: Generated by the parser for each individual code item (function, struct, etc.).
    *   **Storage**: Stored in the database, associated with the specific code item node.

The workflow is as follows:
1.  When a file change is detected, its `FileContentHash` is re-computed.
2.  If the `FileContentHash` has changed, the file is re-parsed.
3.  The `TrackingHash` for each item in the file is re-computed.
4.  If an item's `TrackingHash` has *not* changed, no further action is taken for that item.
5.  If an item's `TrackingHash` *has* changed, expensive re-indexing operations are triggered for that item.

## Consequences
- **Positive**:
    - **Correctness**: Ensures the right hash is used for the right job, preventing mismatches between byte-level and token-level representations.
    - **Performance**: `ploke-io` remains extremely fast, using a highly optimized hashing algorithm (`blake3`) on raw bytes without needing to parse code.
    - **Efficiency**: The system avoids expensive re-indexing for superficial, non-semantic code changes (e.g., adding comments, formatting), saving significant computational resources.
    - **Architectural Clarity**: Enforces a clean separation of concerns. `syn_parser` is responsible for all hashing, and `ploke-io` is a simple, fast I/O worker.

- **Negative**:
    - **Increased Complexity**: The system now manages two hashing mechanisms, which increases the conceptual overhead for developers.
    - **Increased Storage**: An additional 32-byte hash must be stored for every file in the database.
    - **Implementation Scope**: The change is more invasive, touching `ploke-core`, `syn_parser`, `ploke-db`, and `ploke-io`.

- **Neutral**:
    - A small amount of computational overhead is added during the initial parsing phase to generate the second hash, but this is negligible compared to the cost of parsing itself.

## Compliance
- **PROPOSED_ARCH_V3.md Items**: TBD
